{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Model Classes\n",
    "\n",
    "[View notebook on GitHub](https://github.com/BCG-X-Official/artkit/blob/1.0.x/sphinx/source/user_guide/advanced_tutorials/creating_new_model_classes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook serves as a primer for developing ARTKIT model classes. This is most likely your first entrypoint to contributing to ARTKIT. So, to get you started we will cover:\n",
    "\n",
    "- **Object-oriented programming motivations:** The advantages of using class hierarchies and how this enables us to distribute features across client implementations\n",
    "- **ARTKIT's model class hierarchy:** How are classes structured across ARTKIT and what levels of abstraction exist\n",
    "- **Model implementation example:** A deep-dive of the initialization and `get_response` implementation of the `OpenAIChat` class\n",
    "- **Your implementation - a checklist:** Steps to complete to code up your own client implementation\n",
    "\n",
    "This is a technical guide for users who are looking to implement new functionalities within the ARTKIT library. While a basic understanding of python classes is assumed, this guide should be accessible for anyone with a data science background.\n",
    "\n",
    "To get more information on the classes mentioned here go to the [API reference](../../apidoc/artkit.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Object-oriented programming motivations\n",
    "\n",
    "ARTKIT makes heavy use of class inheritance to implement connectors to Gen AI model providers. This allows us to take advantage of features specific to certain model providers while standardizing common utility methods such as asynchronous execution, managing chat history, and caching. It also means you can quickly implement classes to connect to new providers and immediately re-use these standard features without any additional coding.\n",
    "\n",
    "**When should I create a new model class?** Currently ARTKIT provides interfaces for connecting to OpenAI, Anthropic, Hugging Face, Groq, and Google's Gemini LLMs. It also supports OpenAI's multi-modal models, specifically DALL-E models and vision endpoints. If there are additional model providers you'd like to use in your testing and evaluation pipeline, creating a new class is the best way to do so -- and also improves the usefulness of ARTKIT for everyone! To get to know more about contributing to ARTKIT see our [Contributor Guide](../../contributor_guide/index.rst).\n",
    "\n",
    "**What if I want to change core class functionality?** Reviewing the model class hierarchy in this guide is a good starting point to developing new core features to the library, though implementing and testing these features will more take careful considerations and scrutiny, as introducing changes on higher levels of abstraction can have implications for other client implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ARTKIT's model class hierarchy\n",
    "\n",
    "Here's a summary of the model class hierarchy, starting from the abstract `GenAIModel` to the `OpenAIChat` class:\n",
    "\n",
    "1. `GenAIModel` is an abstract representation of a generic Gen AI model\n",
    "2. `ConnectorMixin` is a mixin class that adds a common interface for client connections to a `GenAIModel`\n",
    "3. `ChatModel` is a an abstract class to have a common interface for generating responses for text prompts, emulating a chat - its a subclass of a `GenAIModel`\n",
    "4. `ChatModelConnector` combines the `ConnectorMixin` with the `ChatModel` interface to create chat model that connects to a client\n",
    "5. `OpenAIChat` is a subclass of `ChatModelConnector` to connect to OpenAI's model and utilize them for chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tree of the relevant repo structure, so you can take a look at the implementation of these classes:\n",
    "\n",
    "```\n",
    "model/\n",
    "  base/\n",
    "    _model.py     --> GenAIModel, ConnectorMixin\n",
    "  llm/\n",
    "    base/\n",
    "      _llm.py     --> ChatModel, ChatModelConnector\n",
    "    openai/\n",
    "      _openai.py  --> OpenAIChat\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar hierarchy is shared for other model providers and modalities. For example, levels 1. - 4. are identical for the `AnthropicChat` class. This means that `OpenAIChat` and `AnthropicChat` share all attributes and method signatures of `ChatModelConnector`, such as `get_client()` but not the actual implementation of those methods. This enables model-specific parameters and response processing. \n",
    "\n",
    "***If you're adding a new model class, you will most likely only need to make changes on hierarchy level 5.***\n",
    "\n",
    "It is similar for other modalities; for example, the `OpenAIDiffusion` class inherits from a `DiffusionModelConnector`, which is again a subclass of a `DiffusionModel` and a `ConnectorMixin`. Unlike a `ChatModel`, a `DiffusionModel` does not have a `get_response()` method. However, because it also inherits from `ConnectorMixin`, it will have the same `get_client()` method signature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation example\n",
    "\n",
    "Next, we will demonstrate how the model class hierarchy is used through an example of the `OpenAIChat` class. \n",
    "\n",
    "The goal is to illustrate both (i) what methods you need to implement to create a new model class and (ii) why those methods are set up the way that they are. \n",
    "\n",
    "A `ChatModelConnector` has two required parameters: `model_id` and `api_env_key`. The `OpenAIChat` has additional model-specific parameters such as `temperature`. So when we initialize an `OpenAIChat` object as below, we're also initializing the superclass to handle client connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARTKIT\n",
    "import artkit.api as ak\n",
    "\n",
    "# Load API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "gpt4_chat = ak.OpenAIChat(model_id=\"gpt-4\", api_key_env=\"OPENAI_API_KEY\", temperature=0.5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a chat model connector the following methods need to be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "### model/llm/openai/_openai.py   ###\n",
    "#####################################\n",
    "from typing import Any\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from artkit.model.llm.base import ChatModelConnector\n",
    "from artkit.model.llm.history import ChatHistory\n",
    "\n",
    "\n",
    "class OpenAIChat(ChatModelConnector[AsyncOpenAI]):\n",
    "\n",
    "    # required by ConnectorMixin to establish a client connection\n",
    "    @classmethod\n",
    "    def get_default_api_key_env(cls) -> str:\n",
    "        pass\n",
    "       \n",
    "    # required by ConnectorMixin to establish a client connection\n",
    "    def _make_client(self) -> AsyncOpenAI:  \n",
    "        pass\n",
    "\n",
    "    # required by ChatModel to send a message to the model\n",
    "    async def get_response(  # pragma: no cover\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        history: ChatHistory | None = None,\n",
    "        **model_params: dict[str, Any],\n",
    "    ) -> list[str]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By unifying the interfaces for chat models we are enabled to follow a delegation pattern through which we can add external behaviors to all `ChatModelConnectors` by wrapping them in a separate class. `CachedChatModel` is a great example of this that enables us to cache requests to a model provider without caring about the actual implementation details. You can see us make use of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in a CachedChatModel\n",
    "gpt4_chat = ak.CachedChatModel(\n",
    "    model=gpt4_chat,\n",
    "    database=\"cache/creating_new_model_classes.db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a deeper dive into the `OpenAIChat` implementation of `get_response()` to highlight how we're taking advantage of the full class hierarchy: \n",
    "\n",
    "1. The `with_system_prompt()` method, that you will see later, is inherited from the `ChatModelConnector` superclass to set the `system_prompt` property of the model\n",
    "2. Upon calling `get_response()`, we first use AsyncExitStack() to enter the model's context manager\n",
    "3. Then we call the `get_client()` method of the `ConnectorMixin` superclass to fetch a cached OpenAI client instance.\n",
    "4. If no previous client instance exists, it is created via the model's `_make_client()` implementation. This logic rests in the `ConnectorMixin` as well.\n",
    "5. Next, we format an input message for OpenAI's chat endpoint based on the model's history, system prompt, and input message\n",
    "6. The message is sent to OpenAI's chat endpoint, along with other parameters set during model initialization (such as temperature and max tokens)\n",
    "7. Finally, we parse return a list of responses from OpenAI's chat endpoint\n",
    "\n",
    "While there are quite a few steps here, note that the only ones specific to the `OpenAIChat` class are 5-8. \n",
    "\n",
    "***That means if you're creating a new custom class, all you need to worry about is getting a client instance, passing a message to the client, and returning its response.*** \n",
    "\n",
    "Everything else can be abstracted away via the model superclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### model/llm/openai/_openai.py  ###\n",
    "####################################\n",
    "from typing import Any\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from artkit.model.util import RateLimitException\n",
    "from artkit.model.llm.history import ChatHistory\n",
    "\n",
    "from openai import RateLimitError\n",
    "\n",
    "\n",
    "async def get_response(\n",
    "    self,\n",
    "    message: str,\n",
    "    *,\n",
    "    history: ChatHistory | None = None,\n",
    "    **model_params: dict[str, Any],\n",
    ") -> list[str]:\n",
    "\n",
    "    # ARTKIT model implementations are designed to be fully asynchronous -\n",
    "    #   AsyncExitStack is used to handle multiple context managers dynamically.\n",
    "    async with AsyncExitStack():\n",
    "        try:\n",
    "            # We access the client instance via the get_client method of the \"ConnectorMixin\" superclass - \n",
    "            #   this will fetch a cached client instance if it exists or make a new one if it does not\n",
    "            # This is very helpful, as it means you can share the same client instance across model objects\n",
    "            completion = await self.get_client().chat.completions.create(\n",
    "\n",
    "                # Here is the only OpenAI specific bit of code - we're formatting the message\n",
    "                #  to pass to the chat endpoint\n",
    "                messages=list(\n",
    "                    self._messages_to_openai_format(  # type: ignore[arg-type]\n",
    "                        message, history=history\n",
    "                    )\n",
    "                ),\n",
    "                model=self.model_id,\n",
    "\n",
    "                # We merge the model parameters passed to the get_response method with the defaults set\n",
    "                # during instantiation, by overwriting the defaults with the passed parameters\n",
    "                **{**self.get_model_params(), **model_params},\n",
    "                )\n",
    "        except RateLimitError as e:\n",
    "                # If the rate limit is exceeded, we raise a custom RateLimitException\n",
    "                # This is caught for all ChatModelConnectors and handled via exponential backoff\n",
    "                raise RateLimitException(\n",
    "                    \"Rate limit exceeded. Please try again later.\"\n",
    "                ) from e\n",
    "\n",
    "    return list(self._responses_from_completion(completion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the actual output of our `.get_response()` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue as the endless sea,\n",
      "Reflecting the sun's bright glow,\n",
      "Infinite and free.\n"
     ]
    }
   ],
   "source": [
    "print((await gpt4_chat.with_system_prompt(\"You respond only in haiku\").get_response(message=\"What color is the sky?\"))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Your implementation - a checklist:\n",
    "\n",
    "Here are the basic steps you'll need to take to create a new model class:\n",
    "\n",
    "1. Depending on which kind of model you want to implement the right abstract class e.g., `ChatModelConnect`. Ideally, your IDE assists you here. Otherwise, you can try starting with an existing implementation, but make sure to check your parent classes and method signatures.\n",
    "2. Update `__init__` to only include the parameters relevant for your model\n",
    "3. Update `_make_client` to return an instance of your model's client. To do so, review the model provider's API documentation; refer to [Connecting to Gen AI Models](../introduction_to_artkit/connecting_to_genai_models.ipynb) for some examples of what you're looking for\n",
    "4. Update `get_response` to pass a message to the client endpoint and return its response\n",
    "5. Add unit tests for your new model implementation\n",
    "\n",
    "If you're working with a diffusion or vision model, you will have to implement a different abstract model class but the necessary steps are very similar.\n",
    "\n",
    "Here are a few other best-practices that will save you time during development:\n",
    "\n",
    "- Run pre-commit hooks frequently; they will help you catch any missing implementation, type errors, or general formatting inconsistencies\n",
    "- Write unit tests as you go, and run `pytest` intermittently to make sure you haven't accidentally broken anything\n",
    "- Import your model class in `api.py``; this will allow it to be called via the ARTKIT API\n",
    "- Add a try / expect `ImportError` at the top of your class; ARTKIT does not require every supported model to be installed on setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling custom endpoints via HTTP\n",
    "\n",
    "ARTKIT offers support for sending HTTP requests to any endpoint.  You will need to subclass a version of `HTTPXChatConnector` (which implements the `ChatModelConnector` class).  It has native support for exponential retries if you do not specify any keyword arguments into the constructor (but can be customized to specific retry requirements for the endpoint at hand). Under the hood it uses the `http.AsyncClient` to make requests to your endpoint. You can also pass a dictonary of client key word arguments for example to specify a read timeout:\n",
    "\n",
    "````python\n",
    "from httpx import Timeout\n",
    "\n",
    "client_kwargs = dict(timeout=Timeout(10.0, read=10.0))\n",
    "chat_model = YourImplementedClass(\n",
    "    model_id=\"http://test.url/api/v1/\",\n",
    "    httpx_client_kwargs=client_kwargs,\n",
    ")\n",
    "````\n",
    "\n",
    "With the abstract `HTTPXChatConnector` class most ARTKIT specific implementation detail are already taken care of. All you need to implement are the following functions:\n",
    "\n",
    "````python\n",
    "def get_default_api_key_env(cls) -> str:\n",
    "    # returns the defaul environment variable name\n",
    "    # under which you keep your API Key if needed\n",
    "\n",
    "def build_request_arguments(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        history: ChatHistory | None = None,\n",
    "        **model_params: dict[str, Any],\n",
    "    ) -> dict[str, Any]:\n",
    "    # \"Translates\" the ARTKIT input to an httpx.request input\n",
    "\n",
    "def parse_httpx_response(\n",
    "    self,\n",
    "    response: Response,\n",
    ") -> list[str]\n",
    "    # \"Translates\" the httpx.request output to an ARTKIT output\n",
    "\n",
    "````\n",
    "\n",
    "To make things a bit more concrete we provide an example using the REST API provided by OpenAI but you can also look at the implementations of `ak.HUggingFaceURLChat` and `ak.BaseBedrockChat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "from httpx import Response\n",
    "from artkit.model.llm.base import HTTPXChatConnector\n",
    "from artkit.model.llm.history import ChatHistory\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class OpenAIChatConnector(HTTPXChatConnector):\n",
    "    def __init__(self, model_id: str, model_name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        For this specific example, the OpenAI API endpoint requires a specific model name and thus, a model_name has been added to the constructor.\n",
    "        \"\"\"\n",
    "        super().__init__(model_id=model_id, **kwargs)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    @classmethod\n",
    "    def get_default_api_key_env(cls) -> str:\n",
    "        \"\"\"\n",
    "        Get the default name of the environment variable that holds the API key.\n",
    "\n",
    "        :return: the default name of the api key environment variable\n",
    "        \"\"\"\n",
    "        return \"OPENAI_API_KEY\"\n",
    "\n",
    "\n",
    "    def build_request_arguments(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        history: ChatHistory | None = None,\n",
    "        **model_params: dict[str, Any],\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        This method is responsible for formatting the input to the LLM chat system.\n",
    "        For argument options see :class:`httpx.AsyncClient.request`.\n",
    "\n",
    "        :param message: The input message to format.\n",
    "        :param history: The chat history preceding the message.\n",
    "        :param model_params: Additional parameters for the chat system.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        if self.system_prompt:\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt\n",
    "                }\n",
    "            )\n",
    "        if history:\n",
    "            messages.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": message.role,\n",
    "                        \"content\": message.text,\n",
    "                    }\n",
    "                    for message in history.messages\n",
    "                ]\n",
    "            )\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message\n",
    "            }\n",
    "        )\n",
    "\n",
    "        request_body = json.dumps({\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": messages,\n",
    "            **{**self.get_model_params(), **model_params},\n",
    "        })\n",
    "        return dict(\n",
    "            method=\"POST\",\n",
    "            url=self.model_id,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.get_api_key()}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            data=request_body,\n",
    "        )\n",
    "\n",
    "\n",
    "    def parse_httpx_response(self, response: Response) -> list[str]:\n",
    "        \"\"\"\n",
    "        This method is responsible for formatting the :class:`httpx.Response` after\n",
    "        having made the request.\n",
    "\n",
    "        :param response: The response from the endpoint.\n",
    "        :return: A list of formatted response strings.\n",
    "        \"\"\"\n",
    "        response_json = response.json()\n",
    "        return [response_json['choices'][0]['message']['content']] if 'choices' in response_json else []\n",
    "\n",
    "\n",
    "openai_chat = OpenAIChatConnector(model_id=\"https://api.openai.com/v1/chat/completions\", model_name=\"gpt-4o\")\n",
    "prompt = \"What is 2+2?\"\n",
    "response = await openai_chat.get_response(message=prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock Specifics\n",
    "\n",
    "The AWS Bedrock integration is also based on the `HTTPXCustomChat`.\n",
    "\n",
    "For AWS Bedrock requests, as long as valid credentials are [in-place](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html), ARTKIT will handle that all necessary properties are set for API request signing.  You will need to ensure that that an appropriate AWS account is set-up and that a user within that account has the right IAM policies to access the foundational models that you wish to use.\n",
    "\n",
    "Additionally, you will need to implement specific I/O parsing for calls to those foundational models as each foundational model has distinct request and response payloads (i.e., `build_request_arguments` and `parse_httpx_response`).\n",
    "\n",
    "The goal is to illustrate how we would do that with the existing code.\n",
    "\n",
    "A `BaseBedrockChat` has two required parameters: `model_id` and `region` corresponding to the specific foundational model and region that you wish to make API calls to.  Unlike the other functionality with other components in ARTKIT, the model parameters are submitted via a JSON payload and the `invoke_model` API [see here](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html)\n",
    "\n",
    "To implement a specific payload for another AWS Foundational Model, the following methods will need to be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Claude as an example\n",
    "from typing import Any\n",
    "\n",
    "from artkit.model.llm.bedrock.base import BaseBedrockChat\n",
    "\n",
    "class ClaudeBedrockChat(BaseBedrockChat):\n",
    "    async def build_request_arguments(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        history: ChatHistory | None = None,\n",
    "        **model_params: dict[str, Any],\n",
    "    ) -> dict[str, Any]:\n",
    "        # your implementation here\n",
    "        pass\n",
    "\n",
    "    def parse_httpx_response(self, response: Response) -> list[str]:\n",
    "        # your implementation here\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
